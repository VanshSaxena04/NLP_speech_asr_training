{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0790af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fabf5f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local search dirs: ['/mnt/data', './local_uploads']\n",
      "Reading FT CSV from: FT Data - data.csv\n",
      "Processing rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:08<00:00, 12.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving transcripts_index to data/processed/transcripts_index.json\n",
      "Saving train_manifest to data/processed/train_manifest.jsonl (5941 segments)\n",
      "=== Summary ===\n",
      "Total recordings processed (with transcription parsed): 104\n",
      "Total segments collected: 5941\n",
      "No failures detected.\n",
      "Done. Outputs: {'transcripts_index_path': '/Users/vs/Develop/josh/data/processed/transcripts_index.json', 'train_manifest_path': '/Users/vs/Develop/josh/data/processed/train_manifest.jsonl', 'num_recordings': 104, 'num_segments': 5941, 'failures': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "step1_2_collect_transcripts.py\n",
    "\n",
    "Step 1 & 2:\n",
    "- Read the FT CSV index (FT Data - data.csv)\n",
    "- For each recording:\n",
    "    - find or download the transcription JSON and metadata JSON\n",
    "    - parse transcription segments (start, end, text)\n",
    "    - save raw files locally and create:\n",
    "        - data/processed/transcripts_index.json  (mapping recording_id -> info + segments)\n",
    "        - data/processed/train_manifest.jsonl    (one line per segment, ready for later steps)\n",
    "- Helpful logging and safe failure handling.\n",
    "\n",
    "Usage:\n",
    "    python step1_2_collect_transcripts.py \\\n",
    "        --ft_csv \"FT Data - data.csv\" \\\n",
    "        --out_dir \"./data\" \\\n",
    "        --local_search_dirs \"/mnt/data,./local_uploads\" \\\n",
    "        --download-timeout 30\n",
    "\n",
    "Notes:\n",
    "- Requires: pandas, requests, tqdm\n",
    "- Later steps (disfluency detection and audio clipping) will use the generated manifest files.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# Helper utilities\n",
    "# -----------------------------\n",
    "def is_http_url(s: str) -> bool:\n",
    "    if not isinstance(s, str):\n",
    "        return False\n",
    "    return s.startswith(\"http://\") or s.startswith(\"https://\")\n",
    "\n",
    "def safe_mkdir(path: Path):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def download_http(url: str, dest: Path, timeout: int = 30) -> bool:\n",
    "    \"\"\"Download a file via HTTP(S) and save to dest. Returns True if successful.\"\"\"\n",
    "    try:\n",
    "        # make parent dir\n",
    "        safe_mkdir(dest.parent)\n",
    "        with requests.get(url, stream=True, timeout=timeout) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(dest, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to download {url} -> {dest}: {e}\")\n",
    "        return False\n",
    "\n",
    "def copy_local(src: Path, dest: Path) -> bool:\n",
    "    try:\n",
    "        safe_mkdir(dest.parent)\n",
    "        shutil.copy2(src, dest)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # not fatal, just report\n",
    "        print(f\"[WARN] Failed to copy {src} -> {dest}: {e}\")\n",
    "        return False\n",
    "\n",
    "def find_local_candidate(recording_id: str, local_dirs):\n",
    "    \"\"\"\n",
    "    Look for likely transcription/metadata files for this recording in local_dirs.\n",
    "    Returns Path if found, else None.\n",
    "    Candidate names:\n",
    "      - {recording_id}_transcription.json\n",
    "      - {recording_id}_transcription*.json\n",
    "      - {recording_id}.json\n",
    "    \"\"\"\n",
    "    for d in local_dirs:\n",
    "        p = Path(d)\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        # exact filenames\n",
    "        candidates = [\n",
    "            p / f\"{recording_id}_transcription.json\",\n",
    "            p / f\"{recording_id}.transcription.json\",\n",
    "            p / f\"{recording_id}.json\",\n",
    "            p / f\"{recording_id}_transcript.json\",\n",
    "            p / f\"{recording_id}_transcription\"\n",
    "        ]\n",
    "        # wildcard search\n",
    "        glob_candidates = list(p.glob(f\"*{recording_id}*trans*json\")) + list(p.glob(f\"{recording_id}*.json\"))\n",
    "        for c in candidates + glob_candidates:\n",
    "            if c.exists() and c.is_file():\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def load_json_file(path: Path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to parse JSON {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# Main processing logic\n",
    "# -----------------------------\n",
    "def process_ft_csv(ft_csv_path: Path, out_dir: Path, local_search_dirs, download_timeout=30, dry_run=False):\n",
    "    \"\"\"\n",
    "    Read FT CSV, collect transcriptions and metadata (local or downloaded),\n",
    "    parse segments and write manifest files.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    raw_transcripts_dir = out_dir / \"raw_transcripts\"\n",
    "    raw_metadata_dir = out_dir / \"raw_metadata\"\n",
    "    processed_dir = out_dir / \"processed\"\n",
    "    safe_mkdir(raw_transcripts_dir)\n",
    "    safe_mkdir(raw_metadata_dir)\n",
    "    safe_mkdir(processed_dir)\n",
    "\n",
    "    # read CSV\n",
    "    print(f\"Reading FT CSV from: {ft_csv_path}\")\n",
    "    df = pd.read_csv(ft_csv_path)\n",
    "    required_cols = {\"user_id\", \"recording_id\", \"language\", \"duration\", \"rec_url_gcp\", \"transcription_url_gcp\", \"metadata_url_gcp\"}\n",
    "    if not required_cols.issubset(set(df.columns)):\n",
    "        print(f\"[ERROR] FT CSV missing required columns. Found: {list(df.columns)} Expected at least: {sorted(list(required_cols))}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    transcripts_index = {}   # recording_id -> dict with info + segments\n",
    "    manifest_lines = []      # each element is a dict for a segment (one per segment)\n",
    "\n",
    "    failures = []\n",
    "    print(\"Processing rows...\")\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        rec_id = str(row[\"recording_id\"])\n",
    "        user_id = str(row[\"user_id\"])\n",
    "        lang = row[\"language\"]\n",
    "        duration = float(row[\"duration\"]) if not pd.isna(row[\"duration\"]) else None\n",
    "        rec_url = row[\"rec_url_gcp\"] if not pd.isna(row[\"rec_url_gcp\"]) else None\n",
    "        tx_url = row[\"transcription_url_gcp\"] if not pd.isna(row[\"transcription_url_gcp\"]) else None\n",
    "        md_url = row[\"metadata_url_gcp\"] if not pd.isna(row[\"metadata_url_gcp\"]) else None\n",
    "\n",
    "        # decide local paths to write the raw JSONs\n",
    "        local_tx_path = raw_transcripts_dir / f\"{rec_id}_transcription.json\"\n",
    "        local_md_path = raw_metadata_dir / f\"{rec_id}_metadata.json\"\n",
    "\n",
    "        # 1) Try to find a local transcription file in user-provided local dirs\n",
    "        tx_local_candidate = find_local_candidate(rec_id, local_search_dirs)\n",
    "        if tx_local_candidate:\n",
    "            ok = copy_local(tx_local_candidate, local_tx_path)\n",
    "            if not ok:\n",
    "                print(f\"[WARN] couldn't copy local transcription {tx_local_candidate} for rec {rec_id}\")\n",
    "        else:\n",
    "            # 2) Attempt to download from http(s)\n",
    "            if isinstance(tx_url, str) and is_http_url(tx_url):\n",
    "                if dry_run:\n",
    "                    print(f\"[DRY RUN] Would download transcription: {tx_url} -> {local_tx_path}\")\n",
    "                else:\n",
    "                    ok = download_http(tx_url, local_tx_path, timeout=download_timeout)\n",
    "                    if not ok:\n",
    "                        failures.append((rec_id, \"transcription\", tx_url))\n",
    "            else:\n",
    "                # not an http URL -> skip or recommend gsutil\n",
    "                print(f\"[INFO] transcription URL for {rec_id} is not http(s) or missing. Value: {tx_url}\")\n",
    "                failures.append((rec_id, \"transcription_url_invalid_or_missing\", tx_url))\n",
    "\n",
    "        # 3) metadata (same logic)\n",
    "        md_local_candidate = find_local_candidate(rec_id, local_search_dirs)\n",
    "        if md_local_candidate and \"meta\" in md_local_candidate.name.lower():\n",
    "            copy_local(md_local_candidate, local_md_path)\n",
    "        else:\n",
    "            if isinstance(md_url, str) and is_http_url(md_url):\n",
    "                if dry_run:\n",
    "                    print(f\"[DRY RUN] Would download metadata: {md_url} -> {local_md_path}\")\n",
    "                else:\n",
    "                    ok = download_http(md_url, local_md_path, timeout=download_timeout)\n",
    "                    if not ok:\n",
    "                        failures.append((rec_id, \"metadata\", md_url))\n",
    "            else:\n",
    "                # maybe metadata is in the same local folder with transcription name pattern\n",
    "                # try a heuristic: look for <rec_id>_metadata.json in local_search_dirs\n",
    "                meta_candidate = None\n",
    "                for d in local_search_dirs:\n",
    "                    p = Path(d)\n",
    "                    c = p / f\"{rec_id}_metadata.json\"\n",
    "                    if c.exists():\n",
    "                        meta_candidate = c\n",
    "                if meta_candidate:\n",
    "                    copy_local(meta_candidate, local_md_path)\n",
    "                else:\n",
    "                    # not found\n",
    "                    print(f\"[INFO] no metadata found for {rec_id}. Expected URL: {md_url}\")\n",
    "\n",
    "        # 4) Parse the transcription file if it exists\n",
    "        if local_tx_path.exists():\n",
    "            data = load_json_file(local_tx_path)\n",
    "            if data is None:\n",
    "                failures.append((rec_id, \"parse_transcription_failed\", str(local_tx_path)))\n",
    "                continue\n",
    "\n",
    "            # common transcription shapes:\n",
    "            # - top-level list of segments: [ {start, end, text}, ... ]\n",
    "            # - top-level dict with key 'segments' -> list\n",
    "            # - top-level dict with other shapes; try to extract segments heuristically\n",
    "            segments = None\n",
    "            if isinstance(data, list):\n",
    "                segments = data\n",
    "            elif isinstance(data, dict):\n",
    "                # sometimes there is {'segments': [...]} or {'results':[...]}\n",
    "                if \"segments\" in data and isinstance(data[\"segments\"], list):\n",
    "                    segments = data[\"segments\"]\n",
    "                elif \"results\" in data and isinstance(data[\"results\"], list):\n",
    "                    segments = data[\"results\"]\n",
    "                else:\n",
    "                    # maybe this file itself is one segment dict\n",
    "                    keys = set(data.keys())\n",
    "                    if {\"start\", \"end\", \"text\"}.issubset(keys):\n",
    "                        segments = [data]\n",
    "                    else:\n",
    "                        # try to find nested list values that look like segments\n",
    "                        found = False\n",
    "                        for k, v in data.items():\n",
    "                            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], dict):\n",
    "                                if {\"start\", \"end\", \"text\"}.issubset(set(v[0].keys())):\n",
    "                                    segments = v\n",
    "                                    found = True\n",
    "                                    break\n",
    "                        if not found:\n",
    "                            print(f\"[WARN] transcription JSON has unexpected structure for {rec_id}. We'll store raw but cannot extract segments.\")\n",
    "                            segments = []\n",
    "            else:\n",
    "                print(f\"[WARN] unrecognized transcription json type for {rec_id}: {type(data)}\")\n",
    "                segments = []\n",
    "\n",
    "            # normalize segments: ensure numeric start/end and text present\n",
    "            normalized_segments = []\n",
    "            for i, seg in enumerate(segments):\n",
    "                # skip segments without text\n",
    "                if not isinstance(seg, dict):\n",
    "                    continue\n",
    "                start = seg.get(\"start\") if seg.get(\"start\") is not None else seg.get(\"begin\")\n",
    "                end = seg.get(\"end\") if seg.get(\"end\") is not None else seg.get(\"finish\")\n",
    "                text = seg.get(\"text\") if seg.get(\"text\") is not None else seg.get(\"transcript\") or seg.get(\"content\")\n",
    "                # try to coerce\n",
    "                try:\n",
    "                    start_f = float(start) if start is not None else None\n",
    "                except:\n",
    "                    start_f = None\n",
    "                try:\n",
    "                    end_f = float(end) if end is not None else None\n",
    "                except:\n",
    "                    end_f = None\n",
    "                if text is None:\n",
    "                    # skip empty text segments\n",
    "                    continue\n",
    "                text_str = str(text).strip()\n",
    "                if len(text_str) == 0:\n",
    "                    continue\n",
    "                normalized_segments.append({\n",
    "                    \"segment_id\": f\"{rec_id}_seg_{i:04d}\",\n",
    "                    \"start\": start_f,\n",
    "                    \"end\": end_f,\n",
    "                    \"duration\": (end_f - start_f) if (start_f is not None and end_f is not None) else None,\n",
    "                    \"text\": text_str\n",
    "                })\n",
    "            # write to index\n",
    "            transcripts_index[rec_id] = {\n",
    "                \"recording_id\": rec_id,\n",
    "                \"user_id\": user_id,\n",
    "                \"language\": lang,\n",
    "                \"duration\": duration,\n",
    "                \"audio_url\": rec_url,\n",
    "                \"transcription_local_path\": str(local_tx_path.resolve()),\n",
    "                \"metadata_local_path\": str(local_md_path.resolve()) if local_md_path.exists() else None,\n",
    "                \"num_segments\": len(normalized_segments),\n",
    "                \"segments\": normalized_segments\n",
    "            }\n",
    "\n",
    "            # create manifest lines\n",
    "            for seg in normalized_segments:\n",
    "                manifest_lines.append({\n",
    "                    \"audio_filepath\": rec_url,      # full recording (we will clip later to segment times)\n",
    "                    \"recording_id\": rec_id,\n",
    "                    \"user_id\": user_id,\n",
    "                    \"language\": lang,\n",
    "                    \"segment_id\": seg[\"segment_id\"],\n",
    "                    \"start_time\": seg[\"start\"],\n",
    "                    \"end_time\": seg[\"end\"],\n",
    "                    \"duration\": seg[\"duration\"],\n",
    "                    \"text\": seg[\"text\"]\n",
    "                })\n",
    "        else:\n",
    "            # transcription file missing\n",
    "            failures.append((rec_id, \"transcription_file_missing\", str(local_tx_path)))\n",
    "\n",
    "    # 5) Save outputs\n",
    "    transcripts_index_path = processed_dir / \"transcripts_index.json\"\n",
    "    manifest_path = processed_dir / \"train_manifest.jsonl\"\n",
    "    print(f\"Saving transcripts_index to {transcripts_index_path}\")\n",
    "    with open(transcripts_index_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(transcripts_index, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saving train_manifest to {manifest_path} ({len(manifest_lines)} segments)\")\n",
    "    with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in manifest_lines:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # summary\n",
    "    print(\"=== Summary ===\")\n",
    "    print(f\"Total recordings processed (with transcription parsed): {len(transcripts_index)}\")\n",
    "    print(f\"Total segments collected: {len(manifest_lines)}\")\n",
    "    if failures:\n",
    "        print(f\"Failures or warnings: {len(failures)} (see below sample)\")\n",
    "        for i, fail in enumerate(failures[:10]):\n",
    "            print(f\"  - {fail}\")\n",
    "    else:\n",
    "        print(\"No failures detected.\")\n",
    "\n",
    "    return {\n",
    "        \"transcripts_index_path\": str(transcripts_index_path.resolve()),\n",
    "        \"train_manifest_path\": str(manifest_path.resolve()),\n",
    "        \"num_recordings\": len(transcripts_index),\n",
    "        \"num_segments\": len(manifest_lines),\n",
    "        \"failures\": failures\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Command line interface\n",
    "# -----------------------------\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Collect transcripts from FT CSV, download/copy raw JSONs, and create segment-level manifest.\")\n",
    "    parser.add_argument(\"--ft_csv\", type=str, default=\"FT Data - data.csv\", help=\"/Users/vs/Develop/josh/FT Data - data.csv\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=\"./data\", help=\"Output directory to store raw and processed files\")\n",
    "    parser.add_argument(\"--local_search_dirs\", type=str, default=\"/mnt/data,./local_uploads\", help=\"Comma-separated list of local directories to search for uploaded files\")\n",
    "    parser.add_argument(\"--download_timeout\", type=int, default=30, help=\"Timeout in seconds for HTTP downloads\")\n",
    "    parser.add_argument(\"--dry_run\", action=\"store_true\", help=\"Dry run: do not perform network downloads, just show actions\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "# ... (inside step1_2_collect_transcripts.py)\n",
    "\n",
    "def main():\n",
    "    # --- TEMPORARY FIX: Hardcode the correct path here ---\n",
    "    # Replace the path below with the absolute path to your actual CSV file.\n",
    "    # For example: \n",
    "    # HARDCODED_FT_CSV_PATH = \"/Users/vs/Documents/Josh_Talks_Assignment/FT Data - data.csv\"\n",
    "    HARDCODED_FT_CSV_PATH = \"FT Data - data.csv\" # <-- Adjust this to your file location\n",
    "    \n",
    "    # Check if the hardcoded file exists before proceeding\n",
    "    ft_csv_path = Path(HARDCODED_FT_CSV_PATH)\n",
    "    if not ft_csv_path.exists():\n",
    "        print(f\"[ERROR] Hardcoded FT CSV not found: {ft_csv_path}. Please check the path.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # Proceed with the rest of the script, but use the hardcoded path.\n",
    "    # We will ignore the command-line argument for now to bypass the error.\n",
    "    \n",
    "    args = parse_args() # Still need this to set up other arguments (out_dir, local_search_dirs, etc.)\n",
    "    local_dirs = [d.strip() for d in args.local_search_dirs.split(\",\") if d.strip()]\n",
    "    if \"/mnt/data\" not in local_dirs:\n",
    "        local_dirs = [\"/mnt/data\"] + local_dirs\n",
    "        \n",
    "    print(\"Local search dirs:\", local_dirs)\n",
    "    \n",
    "    # Use the hardcoded path instead of args.ft_csv\n",
    "    result = process_ft_csv(ft_csv_path, Path(args.out_dir), local_dirs, download_timeout=args.download_timeout, dry_run=args.dry_run)\n",
    "    print(\"Done. Outputs:\", result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536ff49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
